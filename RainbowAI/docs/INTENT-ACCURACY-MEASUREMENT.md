# Intent Accuracy â€” How Itâ€™s Measured & Proposals

## Where it appears

- **Dashboard** (`http://localhost:3002/#dashboard`): â€œIntent Accuracyâ€ card shows a single percentage (or â€œ-â€ when no data).
- **Monitor â†’ Performance**: â€œIntent Accuracyâ€ section with overall stats (total / correct / incorrect / rate) and breakdowns by intent, tier, and model.

---

## How itâ€™s measured today

1. **Data source**  
   Every classification is logged in `intent_predictions` (shared DB):
   - `predicted_intent`, `confidence`, `tier`, `model`
   - `was_correct`: `true` | `false` | `null` (null = not yet validated)

2. **When `was_correct` gets set**
   - **Thumbs up (ğŸ‘):** The latest prediction for that conversation is now marked **correct** (`markIntentCorrect`).
   - **Thumbs down (ğŸ‘):** The latest prediction is marked **incorrect** (`markIntentCorrection` with `actualIntent: 'unknown'`).
   - **Staff / escalation:** (Future) Manual correction or escalation can set `actualIntent` and `was_correct` via `markIntentCorrection` with source `manual` or `escalation`.

3. **Formula**
   - **Validated** = rows where `was_correct` is not null (i.e. had feedback or correction).
   - **Intent Accuracy** = `correct / (correct + incorrect)` over validated rows only, as a percentage.
   - If there are no validated rows, the API returns `accuracyRate: null` and the dashboard shows **â€œ-â€**.

4. **API**
   - `GET /api/rainbow/intent/accuracy` â†’ `accuracy.overall.total | correct | incorrect | accuracyRate` and breakdowns `byIntent`, `byTier`, `byModel`.
   - If the DB is unavailable, the route returns empty overall with `accuracyRate: null`.

So: **Intent Accuracy is â€œ% of validated predictions that were correct,â€ where validation comes from thumbs up/down (and later staff correction).**

---

## Why the card sometimes shows â€œ-â€

- **DB not connected:** Intent analytics use the shared PostgreSQL DB; if itâ€™s down, the route returns empty and the card shows â€œ-â€.
- **No validated predictions:** If no one has given ğŸ‘ or ğŸ‘ yet, `correct + incorrect = 0` â†’ `accuracyRate = null` â†’ â€œ-â€.
- **No predictions at all:** No rows in `intent_predictions` (e.g. no traffic or logging off) â†’ same effect.

---

## Proposals (implemented / optional)

### âœ… Implemented: Thumbs up counts as â€œcorrectâ€

- **Before:** Only thumbs down updated `was_correct` (to false). Thumbs up did nothing, so â€œcorrectâ€ was always 0 and the metric was one-sided or 0%.
- **After:** On thumbs up, the latest prediction for that conversation is marked correct via `markIntentCorrect(conversationId)`. So both ğŸ‘ and ğŸ‘ feed into Intent Accuracy.

### Optional: Dashboard card when thereâ€™s no validation

- Keep showing â€œ-â€ when `accuracyRate` is null.
- **Option A â€” Subtitle/tooltip:** e.g. â€œBased on ğŸ‘/ğŸ‘ feedbackâ€ or â€œNeed feedback (ğŸ‘/ğŸ‘) to showâ€ so users know why itâ€™s â€œ-â€.
- **Option B â€” Proxy when validated is low:** e.g. show â€œEst. X%â€ using average confidence of recent predictions, with a tooltip like â€œEstimated from confidence; give feedback for true accuracy.â€ (Can be misleading; use only with clear labeling.)

### Optional: Staff correction

- In Performance or a â€œReviewâ€ tab: for a conversation (or a specific message), let staff choose the â€œactualâ€ intent. Call `markIntentCorrection(conversationId, actualIntent, 'manual')`. That adds validated rows and improves the accuracy denominator without relying only on thumbs.

### Optional: â€œUnvalidatedâ€ count on Performance tab

- Expose `overall.unvalidated` in the UI (e.g. â€œN predictions not yet validatedâ€) so operators see how much of the volume has feedback.

---

## Summary

| What | Detail |
|------|--------|
| **Definition** | Intent Accuracy = correct / (correct + incorrect), in %, over predictions that received feedback or correction. |
| **Correct** | Set when user gives ğŸ‘ (or staff/manual says â€œcorrectâ€). |
| **Incorrect** | Set when user gives ğŸ‘ or staff sets actual intent and it differs from predicted. |
| **â€œ-â€ on dashboard** | No validated data (no feedback yet or DB down). |
| **Implemented** | Thumbs up now marks the last prediction as correct so the dashboard metric is meaningful as feedback accumulates. |
